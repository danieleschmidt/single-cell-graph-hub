"""
Comprehensive Test Runner for Single-Cell Graph Hub.

This test runner executes all test suites and provides comprehensive coverage
and quality metrics as required by the TERRAGON SDLC quality gates.
"""

import sys
import os
import subprocess
import time
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

import scgraph_hub


class QualityGateRunner:
    """Comprehensive quality gate runner with metrics collection."""
    
    def __init__(self):
        self.results = {
            'timestamp': time.time(),
            'test_suites': {},
            'coverage': {},
            'quality_metrics': {},
            'overall_status': 'unknown'
        }
        self.test_files = self._discover_test_files()
        
    def _discover_test_files(self) -> List[str]:
        """Discover all test files in the tests directory."""
        test_dir = Path(__file__).parent
        test_files = []
        
        for test_file in test_dir.glob("test_*.py"):
            if test_file.name != Path(__file__).name:  # Exclude self
                test_files.append(str(test_file))
        
        return test_files
    
    def run_test_suite(self, test_file: str) -> Dict[str, Any]:
        """Run a specific test suite and collect results."""
        print(f"üß™ Running test suite: {Path(test_file).name}")
        
        try:
            # Run the test file
            result = subprocess.run([
                sys.executable, test_file
            ], capture_output=True, text=True, timeout=300)
            
            # Parse results from output
            suite_results = {
                'file': test_file,
                'exit_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'success': result.returncode == 0,
                'tests_run': self._count_tests_in_output(result.stdout),
                'tests_passed': self._count_passed_tests(result.stdout),
                'tests_failed': self._count_failed_tests(result.stdout)
            }
            
            if suite_results['success']:
                print(f"   ‚úÖ PASSED ({suite_results['tests_passed']} tests)")
            else:
                print(f"   ‚ùå FAILED ({suite_results['tests_failed']} failures)")
            
            return suite_results
            
        except subprocess.TimeoutExpired:
            print(f"   ‚è∞ TIMEOUT after 300 seconds")
            return {
                'file': test_file,
                'exit_code': -1,
                'success': False,
                'error': 'timeout',
                'tests_run': 0,
                'tests_passed': 0,
                'tests_failed': 1
            }
        except Exception as e:
            print(f"   üí• ERROR: {e}")
            return {
                'file': test_file,
                'exit_code': -1,
                'success': False,
                'error': str(e),
                'tests_run': 0,
                'tests_passed': 0,
                'tests_failed': 1
            }
    
    def _count_tests_in_output(self, output: str) -> int:
        """Count total number of tests from output."""
        if "Total tests:" in output:
            try:
                line = [l for l in output.split('\n') if "Total tests:" in l][0]
                return int(line.split("Total tests:")[1].strip().split()[0])
            except:
                pass
        
        # Fallback: count test method calls
        return output.count("‚úÖ PASS") + output.count("‚ùå FAIL")
    
    def _count_passed_tests(self, output: str) -> int:
        """Count passed tests from output."""
        if "Passed:" in output:
            try:
                line = [l for l in output.split('\n') if "Passed:" in l][0]
                return int(line.split("Passed:")[1].strip().split()[0])
            except:
                pass
        
        return output.count("‚úÖ PASS")
    
    def _count_failed_tests(self, output: str) -> int:
        """Count failed tests from output."""
        if "Failed:" in output:
            try:
                line = [l for l in output.split('\n') if "Failed:" in l][0]
                return int(line.split("Failed:")[1].strip().split()[0])
            except:
                pass
        
        return output.count("‚ùå FAIL")
    
    def calculate_test_coverage(self) -> Dict[str, Any]:
        """Calculate estimated test coverage metrics."""
        print("üìä Calculating test coverage metrics...")
        
        # Get source files
        src_dir = Path(__file__).parent.parent / "src" / "scgraph_hub"
        source_files = list(src_dir.glob("*.py"))
        
        # Count total lines of code
        total_lines = 0
        total_functions = 0
        
        for src_file in source_files:
            if src_file.name.startswith('__'):
                continue
                
            try:
                with open(src_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    total_lines += len([l for l in lines if l.strip() and not l.strip().startswith('#')])
                    total_functions += len([l for l in lines if l.strip().startswith('def ') or l.strip().startswith('async def ')])
            except Exception as e:
                print(f"   Warning: Could not process {src_file.name}: {e}")
        
        # Count test functions
        test_functions = 0
        for test_file in self.test_files:
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    test_functions += content.count('def test_')
            except Exception:
                pass
        
        # Estimate coverage based on test comprehensiveness
        estimated_coverage = min(95.0, (test_functions / max(total_functions, 1)) * 100)
        
        coverage_metrics = {
            'estimated_coverage_percent': estimated_coverage,
            'total_source_lines': total_lines,
            'total_functions': total_functions,
            'total_test_functions': test_functions,
            'source_files': len(source_files),
            'test_files': len(self.test_files),
            'meets_85_percent_threshold': estimated_coverage >= 85.0
        }
        
        print(f"   üìà Estimated Coverage: {estimated_coverage:.1f}%")
        print(f"   üéØ Meets 85% Threshold: {'Yes' if coverage_metrics['meets_85_percent_threshold'] else 'No'}")
        
        return coverage_metrics
    
    def run_quality_checks(self) -> Dict[str, Any]:
        """Run comprehensive quality checks."""
        print("üîç Running quality checks...")
        
        quality_metrics = {
            'package_importable': False,
            'basic_functionality_works': False,
            'error_handling_robust': False,
            'documentation_complete': False,
            'dependency_handling_graceful': False,
            'performance_optimizations_available': False,
            'security_features_implemented': False
        }
        
        # Test 1: Package Import
        try:
            import scgraph_hub
            quality_metrics['package_importable'] = True
            print("   ‚úÖ Package imports successfully")
        except Exception as e:
            print(f"   ‚ùå Package import failed: {e}")
        
        # Test 2: Basic Functionality
        try:
            catalog = scgraph_hub.get_default_catalog()
            datasets = catalog.list_datasets()
            dataset = scgraph_hub.simple_quick_start("test_dataset", root="./temp_test")
            quality_metrics['basic_functionality_works'] = len(datasets) > 0 and dataset is not None
            print("   ‚úÖ Basic functionality works")
        except Exception as e:
            print(f"   ‚ùå Basic functionality failed: {e}")
        
        # Test 3: Error Handling
        try:
            # Test graceful error handling
            try:
                scgraph_hub.simple_quick_start("invalid_dataset_xyz_nonexistent", root="./temp_test")
                # Should not crash, may warn
                quality_metrics['error_handling_robust'] = True
                print("   ‚úÖ Error handling is robust")
            except Exception:
                print("   ‚ö†Ô∏è  Error handling could be more robust")
        except Exception:
            pass
        
        # Test 4: Documentation
        try:
            # Check for docstrings in main classes
            has_docs = (
                hasattr(scgraph_hub.SimpleSCGraphDataset, '__doc__') and
                scgraph_hub.SimpleSCGraphDataset.__doc__ and
                len(scgraph_hub.SimpleSCGraphDataset.__doc__.strip()) > 10
            )
            quality_metrics['documentation_complete'] = has_docs
            print("   ‚úÖ Documentation is present" if has_docs else "   ‚ö†Ô∏è  Documentation could be improved")
        except Exception:
            pass
        
        # Test 5: Graceful Dependency Handling
        try:
            # Test that advanced features fail gracefully
            try:
                scgraph_hub.PerformanceOptimizer()
                quality_metrics['performance_optimizations_available'] = True
            except ImportError:
                # Expected if dependencies missing
                quality_metrics['dependency_handling_graceful'] = True
            
            print("   ‚úÖ Dependency handling is graceful")
        except Exception:
            pass
        
        # Test 6: Security Features
        try:
            # Test that security features are available
            if hasattr(scgraph_hub, 'SecurityValidator'):
                quality_metrics['security_features_implemented'] = True
                print("   ‚úÖ Security features implemented")
            else:
                print("   ‚ö†Ô∏è  Security features not fully accessible")
        except Exception:
            pass
        
        return quality_metrics
    
    def run_all_tests(self) -> bool:
        """Run all test suites and quality checks."""
        print("üöÄ Starting Comprehensive Quality Gate Testing")
        print("=" * 60)
        print()
        
        # Run all test suites
        total_tests = 0
        total_passed = 0
        total_failed = 0
        
        for test_file in self.test_files:
            suite_result = self.run_test_suite(test_file)
            self.results['test_suites'][Path(test_file).name] = suite_result
            
            total_tests += suite_result.get('tests_run', 0)
            total_passed += suite_result.get('tests_passed', 0)
            total_failed += suite_result.get('tests_failed', 0)
        
        print()
        
        # Calculate coverage
        self.results['coverage'] = self.calculate_test_coverage()
        print()
        
        # Run quality checks
        self.results['quality_metrics'] = self.run_quality_checks()
        print()
        
        # Determine overall status
        success_rate = total_passed / max(total_tests, 1) * 100
        coverage_ok = self.results['coverage']['meets_85_percent_threshold']
        quality_score = sum(self.results['quality_metrics'].values()) / len(self.results['quality_metrics']) * 100
        
        # TERRAGON SDLC Quality Gates
        quality_gates_passed = (
            success_rate >= 90,  # 90%+ test success rate
            coverage_ok,  # 85%+ test coverage
            quality_score >= 70  # 70%+ quality metrics
        )
        
        all_gates_passed = all(quality_gates_passed)
        self.results['overall_status'] = 'PASSED' if all_gates_passed else 'FAILED'
        
        # Print comprehensive results
        print("üìä COMPREHENSIVE TEST RESULTS")
        print("=" * 40)
        print(f"Total Test Suites: {len(self.test_files)}")
        print(f"Total Tests Run: {total_tests}")
        print(f"Tests Passed: {total_passed}")
        print(f"Tests Failed: {total_failed}")
        print(f"Success Rate: {success_rate:.1f}%")
        print()
        
        print("üìà COVERAGE METRICS")
        print("=" * 25)
        coverage = self.results['coverage']
        print(f"Estimated Coverage: {coverage['estimated_coverage_percent']:.1f}%")
        print(f"Source Files: {coverage['source_files']}")
        print(f"Test Files: {coverage['test_files']}")
        print(f"Total Functions: {coverage['total_functions']}")
        print(f"Test Functions: {coverage['total_test_functions']}")
        print()
        
        print("üîç QUALITY METRICS")
        print("=" * 25)
        for metric, value in self.results['quality_metrics'].items():
            status = "‚úÖ" if value else "‚ùå"
            print(f"{status} {metric.replace('_', ' ').title()}")
        print()
        
        print("üéØ TERRAGON SDLC QUALITY GATES")
        print("=" * 35)
        print(f"{'‚úÖ' if quality_gates_passed[0] else '‚ùå'} Test Success Rate ‚â•90%: {success_rate:.1f}%")
        print(f"{'‚úÖ' if quality_gates_passed[1] else '‚ùå'} Test Coverage ‚â•85%: {coverage['estimated_coverage_percent']:.1f}%")
        print(f"{'‚úÖ' if quality_gates_passed[2] else '‚ùå'} Quality Score ‚â•70%: {quality_score:.1f}%")
        print()
        
        print("üèÜ OVERALL RESULT")
        print("=" * 20)
        if all_gates_passed:
            print("üéâ ALL QUALITY GATES PASSED!")
            print("‚ú® Ready for production deployment")
        else:
            print("‚ö†Ô∏è  Some quality gates need attention")
            print("üîß Review failed metrics before deployment")
        
        print()
        
        return all_gates_passed
    
    def save_results(self, output_file: str = "quality_gate_results.json"):
        """Save detailed results to JSON file."""
        try:
            with open(output_file, 'w') as f:
                json.dump(self.results, f, indent=2, default=str)
            print(f"üìÑ Detailed results saved to {output_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not save results: {e}")


def main():
    """Run comprehensive quality gate testing."""
    runner = QualityGateRunner()
    
    try:
        success = runner.run_all_tests()
        runner.save_results()
        
        # Exit with appropriate code
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\nüõë Testing interrupted by user")
        sys.exit(2)
    except Exception as e:
        print(f"\nüí• Testing failed with error: {e}")
        sys.exit(3)


if __name__ == "__main__":
    main()