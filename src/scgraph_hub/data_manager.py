"""Advanced data management system for Single-Cell Graph Hub."""

import os
import logging
import asyncio
import aiofiles
import aiohttp
from typing import Dict, List, Optional, Any, Union, Tuple, AsyncIterator
from pathlib import Path
from datetime import datetime, timedelta
import hashlib
import shutil
import json
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

import torch
import numpy as np
import pandas as pd
from torch_geometric.data import Data, Batch
from torch_geometric.loader import DataLoader

from .database import get_dataset_repository, get_cache_manager, compute_file_checksum
from .catalog import DatasetCatalog
from .preprocessing import preprocess_dataset

logger = logging.getLogger(__name__)


class DataManager:
    """Central data management system for datasets, caching, and processing."""
    
    def __init__(self, 
                 data_root: Optional[str] = None,
                 cache_dir: Optional[str] = None,
                 max_workers: int = 4,
                 enable_async: bool = True):
        """Initialize data manager.
        
        Args:
            data_root: Root directory for data storage
            cache_dir: Directory for caching processed data
            max_workers: Maximum number of worker processes
            enable_async: Whether to enable async operations
        """
        self.data_root = Path(data_root or os.getenv('DATA_ROOT_DIR', './data'))
        self.cache_dir = Path(cache_dir or os.getenv('CACHE_DIR', './cache'))
        self.max_workers = max_workers
        self.enable_async = enable_async
        
        # Create directories
        self.data_root.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self.catalog = DatasetCatalog()
        self.repository = get_dataset_repository()
        self.cache = get_cache_manager()
        
        # Thread/process pools
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        self.process_pool = ProcessPoolExecutor(max_workers=max_workers)
        
        # Download session for async operations
        self._session = None
        
        logger.info(f"DataManager initialized with data_root={self.data_root}, cache_dir={self.cache_dir}")
    
    async def __aenter__(self):
        """Async context manager entry."""
        if self.enable_async:
            self._session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self._session:
            await self._session.close()
        
        self.thread_pool.shutdown(wait=True)
        self.process_pool.shutdown(wait=True)
    
    def get_dataset_path(self, dataset_name: str, file_type: str = 'processed') -> Path:
        """Get path for a dataset file.
        
        Args:\n            dataset_name: Name of the dataset\n            file_type: Type of file ('raw', 'processed', 'cache')\n            \n        Returns:\n            Path to the dataset file\n        \"\"\"\n        if file_type == 'raw':\n            return self.data_root / 'raw' / f\"{dataset_name}.h5ad\"\n        elif file_type == 'processed':\n            return self.data_root / 'processed' / f\"{dataset_name}.pt\"\n        elif file_type == 'cache':\n            return self.cache_dir / f\"{dataset_name}.pt\"\n        else:\n            raise ValueError(f\"Unknown file type: {file_type}\")\n    \n    def is_dataset_available(self, dataset_name: str, file_type: str = 'processed') -> bool:\n        \"\"\"Check if dataset is available locally.\n        \n        Args:\n            dataset_name: Name of the dataset\n            file_type: Type of file to check\n            \n        Returns:\n            True if dataset is available\n        \"\"\"\n        return self.get_dataset_path(dataset_name, file_type).exists()\n    \n    async def download_dataset_async(self, \n                                   dataset_name: str, \n                                   force_redownload: bool = False,\n                                   verify_checksum: bool = True,\n                                   progress_callback: Optional[callable] = None) -> bool:\n        \"\"\"Download dataset asynchronously.\n        \n        Args:\n            dataset_name: Name of the dataset to download\n            force_redownload: Whether to force redownload if file exists\n            verify_checksum: Whether to verify file checksum\n            progress_callback: Callback for download progress\n            \n        Returns:\n            True if download was successful\n        \"\"\"\n        if not self.enable_async or not self._session:\n            # Fallback to sync download\n            return await asyncio.get_event_loop().run_in_executor(\n                self.thread_pool,\n                self._download_dataset_sync,\n                dataset_name, force_redownload, verify_checksum\n            )\n        \n        try:\n            # Get dataset info\n            dataset_info = self.catalog.get_info(dataset_name)\n            download_url = dataset_info.get('url')\n            \n            if not download_url:\n                logger.error(f\"No download URL found for dataset {dataset_name}\")\n                return False\n            \n            # Check if file already exists\n            raw_path = self.get_dataset_path(dataset_name, 'raw')\n            if raw_path.exists() and not force_redownload:\n                logger.info(f\"Dataset {dataset_name} already exists\")\n                return True\n            \n            # Create directory\n            raw_path.parent.mkdir(parents=True, exist_ok=True)\n            \n            # Download file\n            logger.info(f\"Downloading {dataset_name} from {download_url}\")\n            \n            async with self._session.get(download_url) as response:\n                response.raise_for_status()\n                \n                total_size = int(response.headers.get('content-length', 0))\n                downloaded = 0\n                \n                async with aiofiles.open(raw_path, 'wb') as f:\n                    async for chunk in response.content.iter_chunked(8192):\n                        await f.write(chunk)\n                        downloaded += len(chunk)\n                        \n                        if progress_callback and total_size > 0:\n                            progress = downloaded / total_size\n                            progress_callback(dataset_name, progress)\n            \n            # Verify checksum\n            if verify_checksum and 'checksum' in dataset_info:\n                actual_checksum = await asyncio.get_event_loop().run_in_executor(\n                    self.thread_pool, compute_file_checksum, str(raw_path)\n                )\n                \n                expected_checksum = dataset_info['checksum']\n                if actual_checksum != expected_checksum:\n                    logger.error(f\"Checksum verification failed for {dataset_name}\")\n                    raw_path.unlink()  # Remove corrupted file\n                    return False\n            \n            # Update database\n            await asyncio.get_event_loop().run_in_executor(\n                self.thread_pool,\n                self.repository.log_processing_operation,\n                dataset_name, 'download', 'completed'\n            )\n            \n            logger.info(f\"Successfully downloaded {dataset_name}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to download {dataset_name}: {e}\")\n            \n            # Log failure\n            await asyncio.get_event_loop().run_in_executor(\n                self.thread_pool,\n                self.repository.log_processing_operation,\n                dataset_name, 'download', 'failed', error_message=str(e)\n            )\n            \n            return False\n    \n    def _download_dataset_sync(self, \n                              dataset_name: str, \n                              force_redownload: bool = False,\n                              verify_checksum: bool = True) -> bool:\n        \"\"\"Synchronous dataset download.\"\"\"\n        return self.catalog.download_dataset(\n            dataset_name, \n            str(self.get_dataset_path(dataset_name, 'raw').parent),\n            verify_checksum=verify_checksum\n        )\n    \n    async def preprocess_dataset_async(self,\n                                     dataset_name: str,\n                                     preprocessing_config: Optional[Dict[str, Any]] = None,\n                                     force_reprocess: bool = False) -> bool:\n        \"\"\"Preprocess dataset asynchronously.\n        \n        Args:\n            dataset_name: Name of the dataset\n            preprocessing_config: Preprocessing configuration\n            force_reprocess: Whether to force reprocessing\n            \n        Returns:\n            True if preprocessing was successful\n        \"\"\"\n        processed_path = self.get_dataset_path(dataset_name, 'processed')\n        \n        # Check if already processed\n        if processed_path.exists() and not force_reprocess:\n            logger.info(f\"Dataset {dataset_name} already processed\")\n            return True\n        \n        # Ensure raw data is available\n        raw_path = self.get_dataset_path(dataset_name, 'raw')\n        if not raw_path.exists():\n            logger.info(f\"Raw data not found, downloading {dataset_name}\")\n            success = await self.download_dataset_async(dataset_name)\n            if not success:\n                return False\n        \n        # Run preprocessing in process pool\n        try:\n            config = preprocessing_config or {}\n            \n            metadata = await asyncio.get_event_loop().run_in_executor(\n                self.process_pool,\n                preprocess_dataset,\n                dataset_name,\n                str(raw_path),\n                str(processed_path),\n                config.get('steps'),\n                config.get('parameters'),\n                config.get('graph_method', 'knn'),\n                config.get('graph_parameters'),\n                config.get('save_intermediate', False)\n            )\n            \n            logger.info(f\"Successfully preprocessed {dataset_name}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to preprocess {dataset_name}: {e}\")\n            return False\n    \n    def load_dataset(self, \n                    dataset_name: str, \n                    device: str = 'cpu',\n                    use_cache: bool = True) -> Optional[Data]:\n        \"\"\"Load processed dataset.\n        \n        Args:\n            dataset_name: Name of the dataset\n            device: Device to load data on\n            use_cache: Whether to use cache\n            \n        Returns:\n            PyTorch Geometric Data object\n        \"\"\"\n        # Check cache first\n        if use_cache:\n            cache_key = f\"dataset_data:{dataset_name}:{device}\"\n            cached_data = self.cache.get(cache_key)\n            if cached_data:\n                logger.debug(f\"Loading {dataset_name} from cache\")\n                return cached_data\n        \n        # Load from file\n        processed_path = self.get_dataset_path(dataset_name, 'processed')\n        \n        if not processed_path.exists():\n            logger.error(f\"Processed dataset {dataset_name} not found\")\n            return None\n        \n        try:\n            data = torch.load(processed_path, map_location=device)\n            \n            # Cache the data\n            if use_cache:\n                cache_key = f\"dataset_data:{dataset_name}:{device}\"\n                self.cache.set(cache_key, data, ttl=3600)  # 1 hour\n            \n            # Update access tracking\n            self.repository.update_dataset(\n                dataset_name, \n                {'last_accessed': datetime.utcnow()}\n            )\n            \n            return data\n            \n        except Exception as e:\n            logger.error(f\"Failed to load dataset {dataset_name}: {e}\")\n            return None\n    \n    def create_dataloader(self,\n                         dataset_names: Union[str, List[str]],\n                         batch_size: int = 32,\n                         shuffle: bool = True,\n                         num_workers: int = 0,\n                         split: Optional[str] = None,\n                         device: str = 'cpu') -> Optional[DataLoader]:\n        \"\"\"Create PyTorch Geometric DataLoader.\n        \n        Args:\n            dataset_names: Name(s) of datasets to load\n            batch_size: Batch size\n            shuffle: Whether to shuffle data\n            num_workers: Number of data loading workers\n            split: Data split to use ('train', 'val', 'test')\n            device: Device to load data on\n            \n        Returns:\n            DataLoader instance\n        \"\"\"\n        if isinstance(dataset_names, str):\n            dataset_names = [dataset_names]\n        \n        # Load datasets\n        datasets = []\n        for name in dataset_names:\n            data = self.load_dataset(name, device=device)\n            if data is not None:\n                # Handle splits if specified\n                if split and hasattr(data, f'{split}_mask'):\n                    mask = getattr(data, f'{split}_mask')\n                    # Create subset based on mask\n                    # This is simplified - would need proper implementation\n                    datasets.append(data)\n                else:\n                    datasets.append(data)\n            else:\n                logger.warning(f\"Failed to load dataset {name}\")\n        \n        if not datasets:\n            logger.error(\"No datasets loaded successfully\")\n            return None\n        \n        try:\n            return DataLoader(\n                datasets,\n                batch_size=batch_size,\n                shuffle=shuffle,\n                num_workers=num_workers\n            )\n        except Exception as e:\n            logger.error(f\"Failed to create DataLoader: {e}\")\n            return None\n    \n    def get_dataset_statistics(self, dataset_name: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get comprehensive statistics for a dataset.\n        \n        Args:\n            dataset_name: Name of the dataset\n            \n        Returns:\n            Dictionary containing dataset statistics\n        \"\"\"\n        # Check cache first\n        cache_key = f\"dataset_stats:{dataset_name}\"\n        cached_stats = self.cache.get(cache_key)\n        if cached_stats:\n            return cached_stats\n        \n        # Load dataset\n        data = self.load_dataset(dataset_name, use_cache=False)\n        if data is None:\n            return None\n        \n        try:\n            stats = {\n                'basic': {\n                    'num_nodes': data.num_nodes,\n                    'num_edges': data.num_edges,\n                    'num_features': data.num_features if hasattr(data, 'num_features') else data.x.shape[1],\n                    'has_labels': hasattr(data, 'y') and data.y is not None,\n                    'num_classes': len(torch.unique(data.y)) if hasattr(data, 'y') and data.y is not None else 0\n                },\n                'graph': {\n                    'density': data.num_edges / (data.num_nodes * (data.num_nodes - 1)) if data.num_nodes > 1 else 0,\n                    'average_degree': data.num_edges / data.num_nodes if data.num_nodes > 0 else 0,\n                    'is_undirected': self._is_undirected(data.edge_index)\n                },\n                'features': {\n                    'feature_mean': float(torch.mean(data.x)) if hasattr(data, 'x') else None,\n                    'feature_std': float(torch.std(data.x)) if hasattr(data, 'x') else None,\n                    'sparsity': float(torch.mean((data.x == 0).float())) if hasattr(data, 'x') else None\n                }\n            }\n            \n            # Add split information if available\n            if hasattr(data, 'train_mask'):\n                stats['splits'] = {\n                    'train_nodes': int(torch.sum(data.train_mask)),\n                    'val_nodes': int(torch.sum(data.val_mask)) if hasattr(data, 'val_mask') else 0,\n                    'test_nodes': int(torch.sum(data.test_mask)) if hasattr(data, 'test_mask') else 0\n                }\n            \n            # Cache statistics\n            self.cache.set(cache_key, stats, ttl=7200)  # 2 hours\n            \n            return stats\n            \n        except Exception as e:\n            logger.error(f\"Failed to compute statistics for {dataset_name}: {e}\")\n            return None\n    \n    def _is_undirected(self, edge_index: torch.Tensor) -> bool:\n        \"\"\"Check if graph is undirected.\"\"\"\n        from torch_geometric.utils import is_undirected\n        return is_undirected(edge_index)\n    \n    def cleanup_cache(self, max_age_hours: int = 24):\n        \"\"\"Clean up old cached files.\n        \n        Args:\n            max_age_hours: Maximum age of cache files in hours\n        \"\"\"\n        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)\n        \n        for cache_file in self.cache_dir.glob('*.pt'):\n            if cache_file.stat().st_mtime < cutoff_time.timestamp():\n                try:\n                    cache_file.unlink()\n                    logger.debug(f\"Removed old cache file: {cache_file}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to remove cache file {cache_file}: {e}\")\n    \n    def get_storage_info(self) -> Dict[str, Any]:\n        \"\"\"Get storage usage information.\n        \n        Returns:\n            Dictionary with storage information\n        \"\"\"\n        def get_dir_size(path: Path) -> int:\n            \"\"\"Get total size of directory in bytes.\"\"\"\n            total = 0\n            for file in path.rglob('*'):\n                if file.is_file():\n                    total += file.stat().st_size\n            return total\n        \n        try:\n            raw_size = get_dir_size(self.data_root / 'raw') if (self.data_root / 'raw').exists() else 0\n            processed_size = get_dir_size(self.data_root / 'processed') if (self.data_root / 'processed').exists() else 0\n            cache_size = get_dir_size(self.cache_dir) if self.cache_dir.exists() else 0\n            \n            return {\n                'data_root': str(self.data_root),\n                'cache_dir': str(self.cache_dir),\n                'raw_data_size_mb': raw_size / (1024 * 1024),\n                'processed_data_size_mb': processed_size / (1024 * 1024),\n                'cache_size_mb': cache_size / (1024 * 1024),\n                'total_size_mb': (raw_size + processed_size + cache_size) / (1024 * 1024)\n            }\n        except Exception as e:\n            logger.error(f\"Failed to get storage info: {e}\")\n            return {}\n    \n    async def batch_download_datasets(self,\n                                    dataset_names: List[str],\n                                    max_concurrent: int = 3,\n                                    progress_callback: Optional[callable] = None) -> Dict[str, bool]:\n        \"\"\"Download multiple datasets concurrently.\n        \n        Args:\n            dataset_names: List of dataset names to download\n            max_concurrent: Maximum concurrent downloads\n            progress_callback: Callback for progress updates\n            \n        Returns:\n            Dictionary mapping dataset names to success status\n        \"\"\"\n        semaphore = asyncio.Semaphore(max_concurrent)\n        \n        async def download_with_semaphore(name: str) -> Tuple[str, bool]:\n            async with semaphore:\n                success = await self.download_dataset_async(\n                    name, progress_callback=progress_callback\n                )\n                return name, success\n        \n        # Create tasks for all downloads\n        tasks = [download_with_semaphore(name) for name in dataset_names]\n        \n        # Wait for all downloads to complete\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Process results\n        download_results = {}\n        for result in results:\n            if isinstance(result, Exception):\n                logger.error(f\"Download task failed: {result}\")\n                continue\n            \n            name, success = result\n            download_results[name] = success\n        \n        return download_results\n    \n    async def batch_preprocess_datasets(self,\n                                      dataset_names: List[str],\n                                      preprocessing_configs: Optional[Dict[str, Dict[str, Any]]] = None,\n                                      max_concurrent: int = 2) -> Dict[str, bool]:\n        \"\"\"Preprocess multiple datasets concurrently.\n        \n        Args:\n            dataset_names: List of dataset names to preprocess\n            preprocessing_configs: Preprocessing configurations per dataset\n            max_concurrent: Maximum concurrent preprocessing jobs\n            \n        Returns:\n            Dictionary mapping dataset names to success status\n        \"\"\"\n        semaphore = asyncio.Semaphore(max_concurrent)\n        configs = preprocessing_configs or {}\n        \n        async def preprocess_with_semaphore(name: str) -> Tuple[str, bool]:\n            async with semaphore:\n                config = configs.get(name, {})\n                success = await self.preprocess_dataset_async(name, config)\n                return name, success\n        \n        # Create tasks for all preprocessing\n        tasks = [preprocess_with_semaphore(name) for name in dataset_names]\n        \n        # Wait for all preprocessing to complete\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Process results\n        preprocess_results = {}\n        for result in results:\n            if isinstance(result, Exception):\n                logger.error(f\"Preprocessing task failed: {result}\")\n                continue\n            \n            name, success = result\n            preprocess_results[name] = success\n        \n        return preprocess_results\n\n\n# Global data manager instance\n_data_manager = None\n\n\ndef get_data_manager(**kwargs) -> DataManager:\n    \"\"\"Get the global data manager instance.\"\"\"\n    global _data_manager\n    if _data_manager is None:\n        _data_manager = DataManager(**kwargs)\n    return _data_manager\n\n\n# Convenience functions\nasync def download_dataset(dataset_name: str, **kwargs) -> bool:\n    \"\"\"Convenience function to download a dataset.\"\"\"\n    async with get_data_manager() as dm:\n        return await dm.download_dataset_async(dataset_name, **kwargs)\n\n\nasync def preprocess_dataset_async(dataset_name: str, **kwargs) -> bool:\n    \"\"\"Convenience function to preprocess a dataset.\"\"\"\n    async with get_data_manager() as dm:\n        return await dm.preprocess_dataset_async(dataset_name, **kwargs)\n\n\ndef load_dataset(dataset_name: str, **kwargs) -> Optional[Data]:\n    \"\"\"Convenience function to load a dataset.\"\"\"\n    dm = get_data_manager()\n    return dm.load_dataset(dataset_name, **kwargs)\n\n\ndef create_dataloader(dataset_names: Union[str, List[str]], **kwargs) -> Optional[DataLoader]:\n    \"\"\"Convenience function to create a DataLoader.\"\"\"\n    dm = get_data_manager()\n    return dm.create_dataloader(dataset_names, **kwargs)