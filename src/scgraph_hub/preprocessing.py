"""Advanced preprocessing pipelines for single-cell graph data."""

import logging
import warnings
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from pathlib import Path
import numpy as np
import pandas as pd
import torch
from torch_geometric.data import Data

import scanpy as sc
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import KMeans
from scipy.sparse import issparse, csr_matrix, csc_matrix
from scipy.spatial.distance import pdist, squareform
from scipy.stats import zscore
import umap

from .database import get_dataset_repository

logger = logging.getLogger(__name__)


class PreprocessingPipeline:
    """Comprehensive preprocessing pipeline for single-cell data."""
    
    def __init__(self, steps: Optional[List[str]] = None, 
                 parameters: Optional[Dict[str, Any]] = None,
                 track_metadata: bool = True):
        """Initialize preprocessing pipeline.
        
        Args:
            steps: List of preprocessing step names
            parameters: Parameters for each step
            track_metadata: Whether to track processing metadata
        """
        self.steps = steps or self._get_default_steps()
        self.parameters = parameters or {}
        self.track_metadata = track_metadata
        
        # Processing metadata
        self.metadata = {
            'steps_applied': [],
            'parameters_used': {},
            'statistics': {},
            'timing': {}
        }
        
        # Available step functions
        self.step_functions = {
            'filter_cells': self._filter_cells,
            'filter_genes': self._filter_genes,
            'calculate_qc_metrics': self._calculate_qc_metrics,
            'detect_doublets': self._detect_doublets,
            'normalize_total': self._normalize_total,
            'log1p': self._log1p_transform,
            'highly_variable_genes': self._highly_variable_genes,
            'scale': self._scale_data,
            'pca': self._principal_component_analysis,
            'neighbors': self._compute_neighbors,
            'umap': self._compute_umap,
            'clustering': self._perform_clustering,
            'remove_batch_effects': self._remove_batch_effects,
            'impute_missing': self._impute_missing_values,
            'detect_cell_cycle': self._detect_cell_cycle,
            'pseudotime': self._compute_pseudotime
        }
    
    def _get_default_steps(self) -> List[str]:
        """Get default preprocessing steps."""
        return [
            'filter_cells',
            'filter_genes', 
            'calculate_qc_metrics',
            'normalize_total',
            'log1p',
            'highly_variable_genes',
            'scale',
            'pca',
            'neighbors'
        ]
    
    def process(self, adata, return_metadata: bool = False):
        """Run the complete preprocessing pipeline.
        
        Args:
            adata: AnnData object
            return_metadata: Whether to return processing metadata
            
        Returns:
            Processed AnnData object and optionally metadata
        """
        import time
        
        logger.info(f"Starting preprocessing pipeline with {len(self.steps)} steps")
        
        for i, step in enumerate(self.steps):
            logger.info(f"Step {i+1}/{len(self.steps)}: {step}")
            
            start_time = time.time()
            
            if step not in self.step_functions:
                logger.warning(f"Unknown preprocessing step: {step}")
                continue
            
            # Get step parameters
            step_params = self.parameters.get(step, {})\n            \n            try:\n                # Apply step\n                adata = self.step_functions[step](adata, **step_params)\n                \n                # Track metadata\n                if self.track_metadata:\n                    duration = time.time() - start_time\n                    self.metadata['steps_applied'].append(step)\n                    self.metadata['parameters_used'][step] = step_params\n                    self.metadata['timing'][step] = duration\n                    \n                    # Store dataset statistics after each step\n                    self.metadata['statistics'][f'{step}_result'] = {\n                        'n_obs': adata.n_obs,\n                        'n_vars': adata.n_vars,\n                        'sparsity': self._calculate_sparsity(adata.X) if hasattr(adata, 'X') else None\n                    }\n                \n                logger.info(f\"Completed {step} in {time.time() - start_time:.2f}s\")\n                \n            except Exception as e:\n                logger.error(f\"Error in step {step}: {e}\")\n                if not self.parameters.get('continue_on_error', False):\n                    raise\n        \n        logger.info(\"Preprocessing pipeline completed\")\n        \n        if return_metadata:\n            return adata, self.metadata\n        return adata\n    \n    def _calculate_sparsity(self, X) -> float:\n        \"\"\"Calculate sparsity of data matrix.\"\"\"\n        if issparse(X):\n            return 1.0 - X.nnz / (X.shape[0] * X.shape[1])\n        else:\n            return np.mean(X == 0)\n    \n    # Step implementations\n    def _filter_cells(self, adata, min_genes: int = 200, max_genes: Optional[int] = None,\n                     min_counts: Optional[int] = None, max_counts: Optional[int] = None,\n                     mt_gene_pattern: str = '^MT-', max_mt_pct: float = 20.0):\n        \"\"\"Filter cells based on quality metrics.\"\"\"\n        n_cells_before = adata.n_obs\n        \n        # Calculate basic metrics if not present\n        if 'n_genes_by_counts' not in adata.obs.columns:\n            sc.pp.calculate_qc_metrics(adata, percent_top=None, log1p=False, inplace=True)\n        \n        # Calculate mitochondrial gene percentage\n        adata.var['mt'] = adata.var_names.str.match(mt_gene_pattern, case=False)\n        sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n        \n        # Apply filters\n        sc.pp.filter_cells(adata, min_genes=min_genes)\n        \n        if max_genes:\n            sc.pp.filter_cells(adata, max_genes=max_genes)\n        \n        if min_counts:\n            sc.pp.filter_cells(adata, min_counts=min_counts)\n        \n        if max_counts:\n            sc.pp.filter_cells(adata, max_counts=max_counts)\n        \n        # Filter high mitochondrial percentage cells\n        adata = adata[adata.obs.pct_counts_mt < max_mt_pct, :].copy()\n        \n        logger.info(f\"Filtered cells: {n_cells_before} -> {adata.n_obs} ({n_cells_before - adata.n_obs} removed)\")\n        \n        return adata\n    \n    def _filter_genes(self, adata, min_cells: int = 3, max_cells: Optional[int] = None,\n                     min_counts: Optional[int] = None):\n        \"\"\"Filter genes based on expression criteria.\"\"\"\n        n_genes_before = adata.n_vars\n        \n        sc.pp.filter_genes(adata, min_cells=min_cells)\n        \n        if max_cells:\n            sc.pp.filter_genes(adata, max_cells=max_cells)\n        \n        if min_counts:\n            sc.pp.filter_genes(adata, min_counts=min_counts)\n        \n        logger.info(f\"Filtered genes: {n_genes_before} -> {adata.n_vars} ({n_genes_before - adata.n_vars} removed)\")\n        \n        return adata\n    \n    def _calculate_qc_metrics(self, adata, mt_gene_pattern: str = '^MT-',\n                             ribo_gene_pattern: str = '^RP[SL]'):\n        \"\"\"Calculate comprehensive quality control metrics.\"\"\"\n        # Mitochondrial genes\n        adata.var['mt'] = adata.var_names.str.match(mt_gene_pattern, case=False)\n        \n        # Ribosomal genes\n        adata.var['ribo'] = adata.var_names.str.match(ribo_gene_pattern, case=False)\n        \n        # Hemoglobin genes\n        adata.var['hb'] = adata.var_names.str.contains('^HB[^(P)]', case=False)\n        \n        # Calculate metrics\n        sc.pp.calculate_qc_metrics(\n            adata, \n            qc_vars=['mt', 'ribo', 'hb'], \n            percent_top=None, \n            log1p=False, \n            inplace=True\n        )\n        \n        # Additional custom metrics\n        adata.obs['log10_total_counts'] = np.log10(adata.obs['total_counts'])\n        adata.obs['log10_n_genes_by_counts'] = np.log10(adata.obs['n_genes_by_counts'])\n        \n        return adata\n    \n    def _detect_doublets(self, adata, method: str = 'scrublet', **kwargs):\n        \"\"\"Detect potential doublets in the data.\"\"\"\n        try:\n            if method == 'scrublet':\n                import scrublet as scr\n                \n                scrub = scr.Scrublet(adata.X, **kwargs)\n                doublet_scores, predicted_doublets = scrub.scrub_doublets(\n                    min_counts=2, min_cells=3, min_gene_variability_pctl=85,\n                    n_prin_comps=30\n                )\n                \n                adata.obs['doublet_score'] = doublet_scores\n                adata.obs['predicted_doublet'] = predicted_doublets\n                \n                logger.info(f\"Detected {np.sum(predicted_doublets)} potential doublets\")\n                \n            elif method == 'doubletdetection':\n                import doubletdetection as dd\n                \n                clf = dd.BoostClassifier(n_iters=25, use_phenograph=False, **kwargs)\n                doublet_scores = clf.fit(adata.X).predict()\n                \n                adata.obs['doublet_score'] = doublet_scores\n                adata.obs['predicted_doublet'] = doublet_scores > np.percentile(doublet_scores, 90)\n                \n        except ImportError as e:\n            logger.warning(f\"Doublet detection method {method} not available: {e}\")\n        \n        return adata\n    \n    def _normalize_total(self, adata, target_sum: float = 1e4, exclude_highly_expressed: bool = True):\n        \"\"\"Normalize total counts per cell.\"\"\"\n        sc.pp.normalize_total(\n            adata, \n            target_sum=target_sum, \n            exclude_highly_expressed=exclude_highly_expressed\n        )\n        \n        return adata\n    \n    def _log1p_transform(self, adata):\n        \"\"\"Apply log1p transformation.\"\"\"\n        sc.pp.log1p(adata)\n        return adata\n    \n    def _highly_variable_genes(self, adata, n_top_genes: int = 2000, \n                              method: str = 'seurat_v3', batch_key: Optional[str] = None):\n        \"\"\"Identify highly variable genes.\"\"\"\n        if batch_key and batch_key in adata.obs.columns:\n            # Batch-aware HVG selection\n            sc.pp.highly_variable_genes(\n                adata, \n                n_top_genes=n_top_genes, \n                batch_key=batch_key,\n                subset=False\n            )\n        else:\n            sc.pp.highly_variable_genes(\n                adata, \n                n_top_genes=n_top_genes, \n                flavor=method,\n                subset=False\n            )\n        \n        logger.info(f\"Identified {np.sum(adata.var['highly_variable'])} highly variable genes\")\n        \n        return adata\n    \n    def _scale_data(self, adata, max_value: Optional[float] = 10, zero_center: bool = True,\n                   use_highly_variable: bool = True):\n        \"\"\"Scale data to unit variance.\"\"\"\n        # Store raw data\n        adata.raw = adata\n        \n        # Use only highly variable genes if available and requested\n        if use_highly_variable and 'highly_variable' in adata.var.columns:\n            adata = adata[:, adata.var.highly_variable].copy()\n        \n        sc.pp.scale(adata, max_value=max_value, zero_center=zero_center)\n        \n        return adata\n    \n    def _principal_component_analysis(self, adata, n_comps: int = 50, \n                                    use_highly_variable: bool = True,\n                                    svd_solver: str = 'arpack'):\n        \"\"\"Perform principal component analysis.\"\"\"\n        sc.tl.pca(\n            adata, \n            n_comps=n_comps, \n            use_highly_variable=use_highly_variable,\n            svd_solver=svd_solver\n        )\n        \n        return adata\n    \n    def _compute_neighbors(self, adata, n_neighbors: int = 15, n_pcs: Optional[int] = None,\n                          method: str = 'umap', metric: str = 'euclidean'):\n        \"\"\"Compute neighborhood graph.\"\"\"\n        sc.pp.neighbors(\n            adata, \n            n_neighbors=n_neighbors, \n            n_pcs=n_pcs,\n            method=method,\n            metric=metric\n        )\n        \n        return adata\n    \n    def _compute_umap(self, adata, min_dist: float = 0.5, spread: float = 1.0,\n                     n_components: int = 2, alpha: float = 1.0):\n        \"\"\"Compute UMAP embedding.\"\"\"\n        sc.tl.umap(\n            adata,\n            min_dist=min_dist,\n            spread=spread,\n            n_components=n_components,\n            alpha=alpha\n        )\n        \n        return adata\n    \n    def _perform_clustering(self, adata, method: str = 'leiden', \n                           resolution: float = 0.5, **kwargs):\n        \"\"\"Perform clustering analysis.\"\"\"\n        if method == 'leiden':\n            sc.tl.leiden(adata, resolution=resolution, **kwargs)\n        elif method == 'louvain':\n            sc.tl.louvain(adata, resolution=resolution, **kwargs)\n        else:\n            logger.warning(f\"Unknown clustering method: {method}\")\n        \n        return adata\n    \n    def _remove_batch_effects(self, adata, batch_key: str, method: str = 'combat'):\n        \"\"\"Remove batch effects.\"\"\"\n        try:\n            if method == 'combat':\n                sc.pp.combat(adata, key=batch_key)\n            elif method == 'harmony':\n                import scanpy.external as sce\n                sce.pp.harmony_integrate(adata, key=batch_key)\n            elif method == 'scanorama':\n                import scanorama\n                # Implementation for scanorama\n                pass\n            else:\n                logger.warning(f\"Unknown batch correction method: {method}\")\n        \n        except ImportError as e:\n            logger.warning(f\"Batch correction method {method} not available: {e}\")\n        \n        return adata\n    \n    def _impute_missing_values(self, adata, method: str = 'magic', **kwargs):\n        \"\"\"Impute missing values in the data.\"\"\"\n        try:\n            if method == 'magic':\n                import magic\n                magic_op = magic.MAGIC(**kwargs)\n                adata.X = magic_op.fit_transform(adata.X)\n            \n            elif method == 'scimpute':\n                # Placeholder for scImpute implementation\n                logger.warning(\"scImpute not implemented yet\")\n            \n            elif method == 'dca':\n                # Placeholder for DCA implementation\n                logger.warning(\"DCA not implemented yet\")\n        \n        except ImportError as e:\n            logger.warning(f\"Imputation method {method} not available: {e}\")\n        \n        return adata\n    \n    def _detect_cell_cycle(self, adata, organism: str = 'human'):\n        \"\"\"Detect cell cycle phase.\"\"\"\n        try:\n            if organism == 'human':\n                s_genes = ['MCM5', 'PCNA', 'TYMS', 'FEN1', 'MCM2', 'MCM4', 'RRM1', 'UNG', 'GINS2', 'MCM6', 'CDCA7', 'DTL', 'PRIM1', 'UHRF1', 'MLF1IP', 'HELLS', 'RFC2', 'RPA2', 'NASP', 'RAD51AP1', 'GMPS', 'WDR76', 'SLBP', 'CCNE2', 'UBR7', 'POLD3', 'MSH2', 'ATAD2', 'RAD51', 'RRM2', 'CDC45', 'CDC6', 'EXO1', 'TIPIN', 'DSCC1', 'BLM', 'CASP8AP2', 'USP1', 'CLSPN', 'POLA1', 'CHAF1B', 'BRIP1', 'E2F8']\n                g2m_genes = ['HMGB2', 'CDK1', 'NUSAP1', 'UBE2C', 'BIRC5', 'TPX2', 'TOP2A', 'NDC80', 'CKS2', 'NUF2', 'CKS1B', 'MKI67', 'TMPO', 'CENPF', 'TACC3', 'FAM64A', 'SMC4', 'CCNB2', 'CKAP2L', 'CKAP2', 'AURKB', 'BUB1', 'KIF11', 'ANP32E', 'TUBB4B', 'GTSE1', 'KIF20B', 'HJURP', 'CDCA3', 'HN1', 'CDC20', 'TTK', 'CDC25C', 'KIF2C', 'RANGAP1', 'NCAPD2', 'DLGAP5', 'CDCA2', 'CDCA8', 'ECT2', 'KIF23', 'HMMR', 'AURKA', 'PSRC1', 'ANLN', 'LBR', 'CKAP5', 'CENPE', 'CTCF', 'NEK2', 'G2E3', 'GAS2L3', 'CBX5', 'CENPA']\n            elif organism == 'mouse':\n                # Mouse cell cycle genes (converted from human)\n                s_genes = [gene.capitalize() for gene in s_genes]\n                g2m_genes = [gene.capitalize() for gene in g2m_genes]\n            \n            # Score cell cycle\n            sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes)\n            \n        except Exception as e:\n            logger.warning(f\"Cell cycle detection failed: {e}\")\n        \n        return adata\n    \n    def _compute_pseudotime(self, adata, method: str = 'dpt', root_cell: Optional[str] = None):\n        \"\"\"Compute pseudotime using diffusion pseudotime.\"\"\"\n        try:\n            if method == 'dpt':\n                # Compute diffusion pseudotime\n                sc.tl.diffmap(adata)\n                \n                if root_cell:\n                    adata.uns['iroot'] = np.flatnonzero(adata.obs_names == root_cell)[0]\n                else:\n                    # Automatically select root\n                    adata.uns['iroot'] = np.argmin(adata.obsm['X_diffmap'][:, 0])\n                \n                sc.tl.dpt(adata)\n            \n            elif method == 'palantir':\n                # Placeholder for Palantir implementation\n                logger.warning(\"Palantir pseudotime not implemented yet\")\n            \n        except Exception as e:\n            logger.warning(f\"Pseudotime computation failed: {e}\")\n        \n        return adata


class GraphConstructor:\n    \"\"\"Constructs various types of graphs from processed single-cell data.\"\"\"\n    \n    def __init__(self, method: str = 'knn', **parameters):\n        \"\"\"Initialize graph constructor.\n        \n        Args:\n            method: Graph construction method\n            **parameters: Method-specific parameters\n        \"\"\"\n        self.method = method\n        self.parameters = parameters\n        \n        self.methods = {\n            'knn': self._build_knn_graph,\n            'radius': self._build_radius_graph,\n            'spatial': self._build_spatial_graph,\n            'correlation': self._build_correlation_graph,\n            'coexpression': self._build_coexpression_graph,\n            'regulatory': self._build_regulatory_graph\n        }\n    \n    def build_graph(self, adata, return_edge_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Build graph from AnnData object.\n        \n        Args:\n            adata: Processed AnnData object\n            return_edge_weights: Whether to return edge weights\n            \n        Returns:\n            Edge index tensor and optionally edge weights\n        \"\"\"\n        if self.method not in self.methods:\n            raise ValueError(f\"Unknown graph construction method: {self.method}\")\n        \n        return self.methods[self.method](adata, return_edge_weights)\n    \n    def _build_knn_graph(self, adata, return_edge_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Build k-nearest neighbor graph.\"\"\"\n        k = self.parameters.get('k', 15)\n        metric = self.parameters.get('metric', 'euclidean')\n        use_rep = self.parameters.get('use_rep', 'X_pca')\n        \n        # Get representation\n        if use_rep in adata.obsm:\n            X = adata.obsm[use_rep]\n        else:\n            X = adata.X.toarray() if issparse(adata.X) else adata.X\n        \n        # Compute k-NN\n        nbrs = NearestNeighbors(n_neighbors=k+1, metric=metric).fit(X)\n        distances, indices = nbrs.kneighbors(X)\n        \n        # Build edge list\n        edge_list = []\n        edge_weights = []\n        \n        for i in range(len(indices)):\n            for j in range(1, len(indices[i])):  # Skip self\n                neighbor_idx = indices[i][j]\n                edge_list.extend([(i, neighbor_idx), (neighbor_idx, i)])  # Undirected\n                \n                if return_edge_weights:\n                    weight = 1.0 / (1.0 + distances[i][j])  # Distance to similarity\n                    edge_weights.extend([weight, weight])\n        \n        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n        \n        if return_edge_weights:\n            edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n            return edge_index, edge_weights\n        \n        return edge_index, None\n    \n    def _build_radius_graph(self, adata, return_edge_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Build radius-based graph.\"\"\"\n        radius = self.parameters.get('radius', 1.0)\n        metric = self.parameters.get('metric', 'euclidean')\n        use_rep = self.parameters.get('use_rep', 'X_pca')\n        \n        # Get representation\n        if use_rep in adata.obsm:\n            X = adata.obsm[use_rep]\n        else:\n            X = adata.X.toarray() if issparse(adata.X) else adata.X\n        \n        # Compute radius neighbors\n        nbrs = NearestNeighbors(radius=radius, metric=metric).fit(X)\n        distances, indices = nbrs.radius_neighbors(X)\n        \n        edge_list = []\n        edge_weights = []\n        \n        for i, neighbors in enumerate(indices):\n            for j, neighbor_idx in enumerate(neighbors):\n                if i != neighbor_idx:  # Skip self\n                    edge_list.append((i, neighbor_idx))\n                    \n                    if return_edge_weights:\n                        weight = 1.0 / (1.0 + distances[i][j])\n                        edge_weights.append(weight)\n        \n        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n        \n        if return_edge_weights:\n            edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n            return edge_index, edge_weights\n        \n        return edge_index, None\n    \n    def _build_spatial_graph(self, adata, return_edge_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Build spatial proximity graph.\"\"\"\n        if 'spatial' not in adata.obsm:\n            raise ValueError(\"Spatial coordinates not found in adata.obsm['spatial']\")\n        \n        coords = adata.obsm['spatial']\n        max_distance = self.parameters.get('max_distance', 150)\n        method = self.parameters.get('method', 'radius')  # 'radius' or 'delaunay'\n        \n        if method == 'radius':\n            # Radius-based spatial graph\n            nbrs = NearestNeighbors(radius=max_distance).fit(coords)\n            distances, indices = nbrs.radius_neighbors(coords)\n            \n            edge_list = []\n            edge_weights = []\n            \n            for i, neighbors in enumerate(indices):\n                for j, neighbor_idx in enumerate(neighbors):\n                    if i != neighbor_idx:\n                        edge_list.append((i, neighbor_idx))\n                        \n                        if return_edge_weights:\n                            weight = 1.0 / (1.0 + distances[i][j])\n                            edge_weights.append(weight)\n        \n        elif method == 'delaunay':\n            from scipy.spatial import Delaunay\n            \n            # Delaunay triangulation\n            tri = Delaunay(coords)\n            \n            edge_list = []\n            edge_weights = []\n            \n            for simplex in tri.simplices:\n                for i in range(len(simplex)):\n                    for j in range(i+1, len(simplex)):\n                        edge_list.extend([(simplex[i], simplex[j]), (simplex[j], simplex[i])])\n                        \n                        if return_edge_weights:\n                            dist = np.linalg.norm(coords[simplex[i]] - coords[simplex[j]])\n                            if dist <= max_distance:  # Filter long edges\n                                weight = 1.0 / (1.0 + dist)\n                                edge_weights.extend([weight, weight])\n                            else:\n                                edge_list = edge_list[:-2]  # Remove last two edges\n        \n        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n        \n        if return_edge_weights:\n            edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n            return edge_index, edge_weights\n        \n        return edge_index, None\n    \n    def _build_correlation_graph(self, adata, return_edge_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Build correlation-based graph.\"\"\"\n        threshold = self.parameters.get('threshold', 0.7)\n        method = self.parameters.get('method', 'pearson')\n        \n        # Get expression data\n        X = adata.X.toarray() if issparse(adata.X) else adata.X\n        \n        # Compute correlation matrix\n        if method == 'pearson':\n            corr_matrix = np.corrcoef(X)\n        elif method == 'spearman':\n            from scipy.stats import spearmanr\n            corr_matrix, _ = spearmanr(X, axis=1)\n        else:\n            raise ValueError(f\"Unknown correlation method: {method}\")\n        \n        # Create edges where correlation > threshold\n        edge_indices = np.where(corr_matrix > threshold)\n        \n        # Remove self-loops\n        mask = edge_indices[0] != edge_indices[1]\n        edge_list = list(zip(edge_indices[0][mask], edge_indices[1][mask]))\n        \n        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n        \n        if return_edge_weights:\n            edge_weights = torch.tensor(corr_matrix[edge_indices[0][mask], edge_indices[1][mask]], dtype=torch.float)\n            return edge_index, edge_weights\n        \n        return edge_index, None\n    \n    def _build_coexpression_graph(self, adata, return_edge_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Build gene coexpression graph.\"\"\"\n        # This would build edges between genes based on coexpression patterns\n        # Implementation depends on specific requirements\n        logger.warning(\"Coexpression graph construction not fully implemented\")\n        return torch.empty((2, 0), dtype=torch.long), None\n    \n    def _build_regulatory_graph(self, adata, return_edge_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Build gene regulatory network graph.\"\"\"\n        # This would incorporate prior knowledge about gene regulatory relationships\n        # Implementation would require external databases (e.g., STRING, RegNetwork)\n        logger.warning(\"Regulatory graph construction not fully implemented\")\n        return torch.empty((2, 0), dtype=torch.long), None


# Main preprocessing function\ndef preprocess_dataset(dataset_name: str, \n                     input_path: str,\n                     output_path: str,\n                     steps: Optional[List[str]] = None,\n                     parameters: Optional[Dict[str, Any]] = None,\n                     graph_method: str = 'knn',\n                     graph_parameters: Optional[Dict[str, Any]] = None,\n                     save_intermediate: bool = False) -> Dict[str, Any]:\n    \"\"\"Complete preprocessing pipeline for a dataset.\n    \n    Args:\n        dataset_name: Name of the dataset\n        input_path: Path to input H5AD file\n        output_path: Path for output processed file\n        steps: Preprocessing steps to apply\n        parameters: Parameters for preprocessing steps\n        graph_method: Graph construction method\n        graph_parameters: Parameters for graph construction\n        save_intermediate: Whether to save intermediate results\n        \n    Returns:\n        Processing metadata and statistics\n    \"\"\"\n    import time\n    start_time = time.time()\n    \n    logger.info(f\"Starting preprocessing for dataset: {dataset_name}\")\n    \n    try:\n        # Load data\n        adata = sc.read_h5ad(input_path)\n        logger.info(f\"Loaded dataset: {adata.n_obs} cells, {adata.n_vars} genes\")\n        \n        # Initialize preprocessing pipeline\n        pipeline = PreprocessingPipeline(steps=steps, parameters=parameters or {})\n        \n        # Run preprocessing\n        adata, preprocessing_metadata = pipeline.process(adata, return_metadata=True)\n        \n        # Construct graph\n        graph_constructor = GraphConstructor(method=graph_method, **(graph_parameters or {}))\n        edge_index, edge_weights = graph_constructor.build_graph(adata)\n        \n        # Create PyTorch Geometric Data object\n        from torch_geometric.data import Data\n        \n        # Get features\n        if 'X_pca' in adata.obsm:\n            x = torch.FloatTensor(adata.obsm['X_pca'])\n        else:\n            x = torch.FloatTensor(adata.X.toarray() if issparse(adata.X) else adata.X)\n        \n        # Get labels (if available)\n        y = None\n        if 'cell_type' in adata.obs.columns:\n            from sklearn.preprocessing import LabelEncoder\n            le = LabelEncoder()\n            y = torch.LongTensor(le.fit_transform(adata.obs['cell_type']))\n        \n        # Create Data object\n        data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights, y=y)\n        \n        # Save processed data\n        torch.save(data, output_path)\n        \n        # Save AnnData if requested\n        if save_intermediate:\n            adata_path = output_path.replace('.pt', '_adata.h5ad')\n            adata.write(adata_path)\n        \n        # Collect final metadata\n        processing_time = time.time() - start_time\n        \n        final_metadata = {\n            'dataset_name': dataset_name,\n            'processing_time_seconds': processing_time,\n            'preprocessing_steps': preprocessing_metadata['steps_applied'],\n            'preprocessing_parameters': preprocessing_metadata['parameters_used'],\n            'graph_method': graph_method,\n            'graph_parameters': graph_parameters or {},\n            'final_statistics': {\n                'n_cells': adata.n_obs,\n                'n_genes': adata.n_vars,\n                'n_edges': edge_index.shape[1],\n                'graph_density': edge_index.shape[1] / (adata.n_obs * (adata.n_obs - 1)),\n                'feature_dim': x.shape[1] if x is not None else 0,\n                'has_labels': y is not None,\n                'n_classes': len(torch.unique(y)) if y is not None else 0\n            },\n            'file_paths': {\n                'input': input_path,\n                'output': output_path,\n                'adata': adata_path if save_intermediate else None\n            }\n        }\n        \n        # Log to database if available\n        try:\n            repo = get_dataset_repository()\n            repo.log_processing_operation(\n                dataset_name=dataset_name,\n                operation='preprocess',\n                status='completed',\n                duration_seconds=processing_time,\n                parameters={\n                    'preprocessing_steps': steps,\n                    'preprocessing_parameters': parameters,\n                    'graph_method': graph_method,\n                    'graph_parameters': graph_parameters\n                },\n                results=final_metadata['final_statistics']\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to log to database: {e}\")\n        \n        logger.info(f\"Preprocessing completed successfully in {processing_time:.2f}s\")\n        \n        return final_metadata\n        \n    except Exception as e:\n        logger.error(f\"Preprocessing failed: {e}\")\n        \n        # Log failure to database\n        try:\n            repo = get_dataset_repository()\n            repo.log_processing_operation(\n                dataset_name=dataset_name,\n                operation='preprocess',\n                status='failed',\n                error_message=str(e)\n            )\n        except Exception:\n            pass\n        \n        raise